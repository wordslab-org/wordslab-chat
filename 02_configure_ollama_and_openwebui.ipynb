{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6b84757-df87-4a8e-b8b4-6374231e4681",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T22:27:26.140363Z",
     "iopub.status.busy": "2024-10-09T22:27:26.139876Z",
     "iopub.status.idle": "2024-10-09T22:27:26.145878Z",
     "shell.execute_reply": "2024-10-09T22:27:26.144964Z",
     "shell.execute_reply.started": "2024-10-09T22:27:26.140301Z"
    }
   },
   "source": [
    "# wordslab-chat - Configure ollama and Open WebUI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a957aaa8-bda3-4c05-a60a-e0549295cc63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T22:27:47.459382Z",
     "iopub.status.busy": "2024-10-09T22:27:47.458492Z",
     "iopub.status.idle": "2024-10-09T22:27:47.463870Z",
     "shell.execute_reply": "2024-10-09T22:27:47.462674Z",
     "shell.execute_reply.started": "2024-10-09T22:27:47.459342Z"
    }
   },
   "source": [
    "## Configure ollama\n",
    "\n",
    "https://github.com/ollama/ollama/blob/main/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42730fa-c6b5-4e88-b7c4-1434d9c4184d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T22:32:40.315073Z",
     "iopub.status.busy": "2024-10-09T22:32:40.314634Z",
     "iopub.status.idle": "2024-10-09T22:32:40.321119Z",
     "shell.execute_reply": "2024-10-09T22:32:40.320382Z",
     "shell.execute_reply.started": "2024-10-09T22:32:40.315033Z"
    }
   },
   "source": [
    "### Launch local ollama server\n",
    "\n",
    "https://github.com/ollama/ollama/blob/main/README.md#start-ollama\n",
    "\n",
    "https://github.com/ollama/ollama/blob/cd7e01e8b9bddf343e48484fef59e9cfd21dbff7/envconfig/config.go#L232"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "154dd876-67c2-440e-87c1-b97d90026135",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T23:35:49.233596Z",
     "iopub.status.busy": "2024-10-09T23:35:49.232892Z",
     "iopub.status.idle": "2024-10-09T23:35:49.358329Z",
     "shell.execute_reply": "2024-10-09T23:35:49.356107Z",
     "shell.execute_reply.started": "2024-10-09T23:35:49.233552Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the local directory where ollama models will be stored\n",
    "!mkdir -p /models/ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "001a8ec9-b5cf-400e-b664-b2101b6d32f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T23:33:19.787901Z",
     "iopub.status.busy": "2024-10-09T23:33:19.787680Z",
     "iopub.status.idle": "2024-10-09T23:33:19.793459Z",
     "shell.execute_reply": "2024-10-09T23:33:19.792691Z",
     "shell.execute_reply.started": "2024-10-09T23:33:19.787884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find '/root/.ollama/id_ed25519'. Generating new private key.\n",
      "Your new public key is: \n",
      "\n",
      "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIC7Qr4QgFTqxr+d8QyUq1t+CltAK2bgWNBBEg4Fh3f7z\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/10 01:33:19 routes.go:1153: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:true OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/models/ollama OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[* http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\n",
      "time=2024-10-10T01:33:19.870+02:00 level=INFO source=images.go:753 msg=\"total blobs: 5\"\n",
      "time=2024-10-10T01:33:19.870+02:00 level=INFO source=images.go:760 msg=\"total unused blobs removed: 0\"\n",
      "time=2024-10-10T01:33:19.870+02:00 level=INFO source=routes.go:1200 msg=\"Listening on [::]:11434 (version 0.3.12)\"\n",
      "time=2024-10-10T01:33:19.872+02:00 level=INFO source=common.go:135 msg=\"extracting embedded files\" dir=/tmp/ollama15922153/runners\n",
      "time=2024-10-10T01:33:30.967+02:00 level=INFO source=common.go:49 msg=\"Dynamic LLM libraries\" runners=\"[cpu_avx cpu_avx2 cuda_v11 cuda_v12 rocm_v60102 cpu]\"\n",
      "time=2024-10-10T01:33:30.968+02:00 level=INFO source=gpu.go:199 msg=\"looking for compatible GPUs\"\n",
      "time=2024-10-10T01:33:34.506+02:00 level=INFO source=types.go:107 msg=\"inference compute\" id=GPU-91a84793-df17-e5db-d730-1a127defe3a2 library=cuda variant=v12 compute=8.6 driver=12.7 name=\"NVIDIA GeForce RTX 3070 Ti Laptop GPU\" total=\"8.0 GiB\" available=\"6.9 GiB\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Configure ollama server\n",
    "env = os.environ.copy()\n",
    "env[\"OLLAMA_HOST\"]=\"0.0.0.0\"\n",
    "env[\"OLLAMA_ORIGINS\"]=\"*\"\n",
    "env[\"OLLAMA_MODELS\"]=\"/models/ollama\"\n",
    "env[\"OLLAMA_FLASH_ATTENTION\"]=\"1\"\n",
    "\n",
    "# Start ollama server in the background\n",
    "process = subprocess.Popen(['ollama', 'serve'], env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928b7fb5-61cd-4d92-9729-09d9fc0a5020",
   "metadata": {},
   "source": [
    "### Download llama 3.1 8b [4.7 BG]\n",
    "\n",
    "https://github.com/ollama/ollama/blob/main/README.md#model-library\n",
    "\n",
    "https://github.com/ollama/ollama/blob/main/README.md#pull-a-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d64b785-e207-4761-b73c-caab0f4fb6bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T22:48:06.370358Z",
     "iopub.status.busy": "2024-10-09T22:48:06.370134Z",
     "iopub.status.idle": "2024-10-09T22:48:07.277214Z",
     "shell.execute_reply": "2024-10-09T22:48:07.275432Z",
     "shell.execute_reply.started": "2024-10-09T22:48:06.370338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2024/10/10 - 00:48:06 | 200 |      26.701µs |       127.0.0.1 | HEAD     \"/\"\n",
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h[GIN] 2024/10/10 - 00:48:07 | 200 |  760.209528ms |       127.0.0.1 | POST     \"/api/pull\"\n",
      "\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 8eeb52dfb3bb... 100% ▕████████████████▏ 4.7 GB                         \n",
      "pulling 948af2743fc7... 100% ▕████████████████▏ 1.5 KB                         \n",
      "pulling 0ba8f0e314b4... 100% ▕████████████████▏  12 KB                         \n",
      "pulling 56bb8bd477a5... 100% ▕████████████████▏   96 B                         \n",
      "pulling 1a4c3c319823... 100% ▕████████████████▏  485 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f68ddf60-7cbc-4ecd-8edc-16e6a2bc012a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T22:48:14.642077Z",
     "iopub.status.busy": "2024-10-09T22:48:14.641739Z",
     "iopub.status.idle": "2024-10-09T22:48:14.772394Z",
     "shell.execute_reply": "2024-10-09T22:48:14.770611Z",
     "shell.execute_reply.started": "2024-10-09T22:48:14.642050Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2024/10/10 - 00:48:14 | 200 |      21.983µs |       127.0.0.1 | HEAD     \"/\"\n",
      "[GIN] 2024/10/10 - 00:48:14 | 200 |     243.832µs |       127.0.0.1 | GET      \"/api/tags\"\n",
      "NAME               ID              SIZE      MODIFIED      \n",
      "llama3.1:latest    42182419e950    4.7 GB    7 seconds ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a79c50-0686-49ca-bde6-9064ec06b003",
   "metadata": {},
   "source": [
    "## Configure Open WebUI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a432710-7554-4c11-a243-a4a41c1d2651",
   "metadata": {},
   "source": [
    "### Launch local Open WebUI container\n",
    "\n",
    "https://docs.openwebui.com/getting-started/#installation-with-default-configuration\n",
    "\n",
    "https://docs.openwebui.com/getting-started/env-configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71b0e2c0-bdf6-4ee4-ba1b-e47830dfa3d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T23:41:48.371781Z",
     "iopub.status.busy": "2024-10-09T23:41:48.371378Z",
     "iopub.status.idle": "2024-10-09T23:41:48.496732Z",
     "shell.execute_reply": "2024-10-09T23:41:48.494400Z",
     "shell.execute_reply.started": "2024-10-09T23:41:48.371742Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the local directory where Open WebUI data will be stored\n",
    "!mkdir -p /workspace/wordslab-chat/open-webui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54c4f303-149d-4447-849a-2ec621896aad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T23:42:39.689277Z",
     "iopub.status.busy": "2024-10-09T23:42:39.687801Z",
     "iopub.status.idle": "2024-10-09T23:42:40.426160Z",
     "shell.execute_reply": "2024-10-09T23:42:40.425292Z",
     "shell.execute_reply.started": "2024-10-09T23:42:39.689232Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47cd9e84b0a5899e1d5f01baddb2504b9e12db8cf77b60f78ed44e6a95af93dd\n",
      "[GIN] 2024/10/10 - 01:42:59 | 200 |    2.719028ms |      172.17.0.2 | GET      \"/api/tags\"\n",
      "[GIN] 2024/10/10 - 01:43:00 | 200 |     185.927µs |      172.17.0.2 | GET      \"/api/tags\"\n",
      "[GIN] 2024/10/10 - 01:43:00 | 200 |      43.671µs |      172.17.0.2 | GET      \"/api/version\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2024-10-10T01:46:25.355+02:00 level=INFO source=sched.go:714 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/models/ollama/blobs/sha256-8eeb52dfb3bb9aefdf9d1ef24b3bdbcfbe82238798c4b918278320b6fcef18fe gpu=GPU-91a84793-df17-e5db-d730-1a127defe3a2 parallel=4 available=7444889600 required=\"6.2 GiB\"\n",
      "time=2024-10-10T01:46:25.356+02:00 level=INFO source=server.go:103 msg=\"system memory\" total=\"29.4 GiB\" free=\"27.6 GiB\" free_swap=\"8.0 GiB\"\n",
      "time=2024-10-10T01:46:25.356+02:00 level=INFO source=memory.go:326 msg=\"offload to cuda\" layers.requested=-1 layers.model=33 layers.offload=33 layers.split=\"\" memory.available=\"[6.9 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"6.2 GiB\" memory.required.partial=\"6.2 GiB\" memory.required.kv=\"1.0 GiB\" memory.required.allocations=\"[6.2 GiB]\" memory.weights.total=\"4.7 GiB\" memory.weights.repeating=\"4.3 GiB\" memory.weights.nonrepeating=\"411.0 MiB\" memory.graph.full=\"560.0 MiB\" memory.graph.partial=\"677.5 MiB\"\n",
      "time=2024-10-10T01:46:25.372+02:00 level=INFO source=server.go:388 msg=\"starting llama server\" cmd=\"/tmp/ollama15922153/runners/cuda_v12/ollama_llama_server --model /models/ollama/blobs/sha256-8eeb52dfb3bb9aefdf9d1ef24b3bdbcfbe82238798c4b918278320b6fcef18fe --ctx-size 8192 --batch-size 512 --embedding --log-disable --n-gpu-layers 33 --flash-attn --parallel 4 --port 42973\"\n",
      "time=2024-10-10T01:46:25.373+02:00 level=INFO source=sched.go:449 msg=\"loaded runners\" count=1\n",
      "time=2024-10-10T01:46:25.374+02:00 level=INFO source=server.go:587 msg=\"waiting for llama runner to start responding\"\n",
      "time=2024-10-10T01:46:25.375+02:00 level=INFO source=server.go:621 msg=\"waiting for server to become available\" status=\"llm server error\"\n",
      "llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /models/ollama/blobs/sha256-8eeb52dfb3bb9aefdf9d1ef24b3bdbcfbe82238798c4b918278320b6fcef18fe (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   6:                            general.license str              = llama3.1\n",
      "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   9:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO [main] build info | build=10 commit=\"188ed47\" tid=\"140534843723776\" timestamp=1728517585\n",
      "INFO [main] system info | n_threads=10 n_threads_batch=10 system_info=\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \" tid=\"140534843723776\" timestamp=1728517585 total_threads=20\n",
      "INFO [main] HTTP server listening | hostname=\"127.0.0.1\" n_threads_http=\"19\" port=\"42973\" tid=\"140534843723776\" timestamp=1728517585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "time=2024-10-10T01:46:25.627+02:00 level=INFO source=server.go:621 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   66 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) \n",
      "llm_load_print_meta: general.name     = Meta Llama 3.1 8B Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3070 Ti Laptop GPU, compute capability 8.6, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   281.81 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  4156.00 MiB\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 1\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     2.02 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    24.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 903\n",
      "llama_new_context_with_model: graph splits = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO [main] model loaded | tid=\"140534843723776\" timestamp=1728517593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2024-10-10T01:46:33.670+02:00 level=INFO source=server.go:626 msg=\"llama runner started in 8.30 seconds\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2024/10/10 - 01:48:16 | 200 |         1m52s |      172.17.0.2 | POST     \"/api/chat\"\n",
      "[GIN] 2024/10/10 - 01:48:20 | 200 |  3.598932857s |      172.17.0.2 | POST     \"/api/chat\"\n"
     ]
    }
   ],
   "source": [
    "!docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v /workspace/wordslab-chat/open-webui:/app/backend/data -e WEBUI_AUTH=False --name open-webui --restart always ghcr.io/open-webui/open-webui:main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1df7ab0-2928-4205-9a2a-4cd62b61090c",
   "metadata": {},
   "source": [
    "### Configure Open WebUI using the admin panel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827e6323-9435-4d9a-95e8-c6f321002167",
   "metadata": {},
   "source": [
    "1. Wait 30 seconds to one minute to leave a little bit of time for Open WebUI container initialization.\n",
    "\n",
    "2. Open your browser to the following URL\n",
    "\n",
    "http://localhost:3000\n",
    "\n",
    "3. Select the Llama 3.1 model at the top left of the screen.\n",
    "   \n",
    "4. Type a question and make sure the dialog works (you will have to wait 10 seconds while ollama is loading the model on the GPU on the first request)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d94e9e-e890-40d0-ade9-141542f79a03",
   "metadata": {},
   "source": [
    "## Stop local servers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba216cdd-a886-49da-88cb-78631a71d0f5",
   "metadata": {},
   "source": [
    "### Stop Open WebUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aafbaaaf-3a83-4ab0-8e80-2382ac893b13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T23:48:47.659988Z",
     "iopub.status.busy": "2024-10-09T23:48:47.659371Z",
     "iopub.status.idle": "2024-10-09T23:48:50.096885Z",
     "shell.execute_reply": "2024-10-09T23:48:50.096213Z",
     "shell.execute_reply.started": "2024-10-09T23:48:47.659966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open-webui\n"
     ]
    }
   ],
   "source": [
    "!docker stop open-webui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407cbe8b-9aff-4036-b5c7-d81c93c1e250",
   "metadata": {},
   "source": [
    "### Stop ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb3e60e9-2aa8-4879-9f5f-50197c9c0f3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T23:48:52.122374Z",
     "iopub.status.busy": "2024-10-09T23:48:52.121807Z",
     "iopub.status.idle": "2024-10-09T23:48:53.009537Z",
     "shell.execute_reply": "2024-10-09T23:48:53.008752Z",
     "shell.execute_reply.started": "2024-10-09T23:48:52.122322Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import signal\n",
    "\n",
    "os.kill(process.pid, signal.SIGINT)\n",
    "process.wait() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952da3f0-e0a2-442d-bcfe-04da4820c7ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wordslab-chat",
   "language": "python",
   "name": "wordslab-chat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
